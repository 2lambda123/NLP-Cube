#!/bin/bash
python3 cube/networks/tokenizer.py --batch-size=16 --device=cuda:0 --store=data/tokenizer-afrikaans --train=examples/multilanguage/languages/afrikaans.json
python3 cube/networks/tokenizer.py --batch-size=16 --device=cuda:0 --store=data/tokenizer-ancient_greek --train=examples/multilanguage/languages/ancient_greek.json
python3 cube/networks/tokenizer.py --batch-size=16 --device=cuda:0 --store=data/tokenizer-arabic --train=examples/multilanguage/languages/arabic.json
python3 cube/networks/tokenizer.py --batch-size=16 --device=cuda:0 --store=data/tokenizer-armenian --train=examples/multilanguage/languages/armenian.json
python3 cube/networks/tokenizer.py --batch-size=16 --device=cuda:0 --store=data/tokenizer-basque --train=examples/multilanguage/languages/basque.json
python3 cube/networks/tokenizer.py --batch-size=16 --device=cuda:0 --store=data/tokenizer-belarusian --train=examples/multilanguage/languages/belarusian.json
python3 cube/networks/tokenizer.py --batch-size=16 --device=cuda:0 --store=data/tokenizer-bulgarian --train=examples/multilanguage/languages/bulgarian.json
python3 cube/networks/tokenizer.py --batch-size=16 --device=cuda:0 --store=data/tokenizer-buryat --train=examples/multilanguage/languages/buryat.json
python3 cube/networks/tokenizer.py --batch-size=16 --device=cuda:0 --store=data/tokenizer-catalan --train=examples/multilanguage/languages/catalan.json
python3 cube/networks/tokenizer.py --batch-size=16 --device=cuda:0 --store=data/tokenizer-chinese --train=examples/multilanguage/languages/chinese.json
python3 cube/networks/tokenizer.py --batch-size=16 --device=cuda:0 --store=data/tokenizer-classical_chinese --train=examples/multilanguage/languages/classical_chinese.json
python3 cube/networks/tokenizer.py --batch-size=16 --device=cuda:0 --store=data/tokenizer-coptic --train=examples/multilanguage/languages/coptic.json
python3 cube/networks/tokenizer.py --batch-size=16 --device=cuda:0 --store=data/tokenizer-croatian --train=examples/multilanguage/languages/croatian.json
python3 cube/networks/tokenizer.py --batch-size=16 --device=cuda:0 --store=data/tokenizer-czech --train=examples/multilanguage/languages/czech.json
python3 cube/networks/tokenizer.py --batch-size=16 --device=cuda:0 --store=data/tokenizer-danish --train=examples/multilanguage/languages/danish.json
python3 cube/networks/tokenizer.py --batch-size=16 --device=cuda:0 --store=data/tokenizer-dutch --train=examples/multilanguage/languages/dutch.json
python3 cube/networks/tokenizer.py --batch-size=16 --device=cuda:0 --store=data/tokenizer-english --train=examples/multilanguage/languages/english.json
python3 cube/networks/tokenizer.py --batch-size=16 --device=cuda:0 --store=data/tokenizer-estonian --train=examples/multilanguage/languages/estonian.json
python3 cube/networks/tokenizer.py --batch-size=16 --device=cuda:0 --store=data/tokenizer-finnish --train=examples/multilanguage/languages/finnish.json
python3 cube/networks/tokenizer.py --batch-size=16 --device=cuda:0 --store=data/tokenizer-french --train=examples/multilanguage/languages/french.json
python3 cube/networks/tokenizer.py --batch-size=16 --device=cuda:0 --store=data/tokenizer-galician --train=examples/multilanguage/languages/galician.json
python3 cube/networks/tokenizer.py --batch-size=16 --device=cuda:0 --store=data/tokenizer-german --train=examples/multilanguage/languages/german.json
python3 cube/networks/tokenizer.py --batch-size=16 --device=cuda:0 --store=data/tokenizer-gothic --train=examples/multilanguage/languages/gothic.json
python3 cube/networks/tokenizer.py --batch-size=16 --device=cuda:0 --store=data/tokenizer-greek --train=examples/multilanguage/languages/greek.json
python3 cube/networks/tokenizer.py --batch-size=16 --device=cuda:0 --store=data/tokenizer-hebrew --train=examples/multilanguage/languages/hebrew.json
python3 cube/networks/tokenizer.py --batch-size=16 --device=cuda:0 --store=data/tokenizer-hindi --train=examples/multilanguage/languages/hindi.json
python3 cube/networks/tokenizer.py --batch-size=16 --device=cuda:0 --store=data/tokenizer-hungarian --train=examples/multilanguage/languages/hungarian.json
python3 cube/networks/tokenizer.py --batch-size=16 --device=cuda:0 --store=data/tokenizer-indonesian --train=examples/multilanguage/languages/indonesian.json
python3 cube/networks/tokenizer.py --batch-size=16 --device=cuda:0 --store=data/tokenizer-irish --train=examples/multilanguage/languages/irish.json
python3 cube/networks/tokenizer.py --batch-size=16 --device=cuda:0 --store=data/tokenizer-italian --train=examples/multilanguage/languages/italian.json
python3 cube/networks/tokenizer.py --batch-size=16 --device=cuda:0 --store=data/tokenizer-japanese --train=examples/multilanguage/languages/japanese.json
python3 cube/networks/tokenizer.py --batch-size=16 --device=cuda:0 --store=data/tokenizer-kazakh --train=examples/multilanguage/languages/kazakh.json
python3 cube/networks/tokenizer.py --batch-size=16 --device=cuda:0 --store=data/tokenizer-korean --train=examples/multilanguage/languages/korean.json
python3 cube/networks/tokenizer.py --batch-size=16 --device=cuda:0 --store=data/tokenizer-kurmanji --train=examples/multilanguage/languages/kurmanji.json
python3 cube/networks/tokenizer.py --batch-size=16 --device=cuda:0 --store=data/tokenizer-latin --train=examples/multilanguage/languages/latin.json
python3 cube/networks/tokenizer.py --batch-size=16 --device=cuda:0 --store=data/tokenizer-latvian --train=examples/multilanguage/languages/latvian.json
python3 cube/networks/tokenizer.py --batch-size=16 --device=cuda:0 --store=data/tokenizer-lithuanian --train=examples/multilanguage/languages/lithuanian.json
python3 cube/networks/tokenizer.py --batch-size=16 --device=cuda:0 --store=data/tokenizer-livvi --train=examples/multilanguage/languages/livvi.json
python3 cube/networks/tokenizer.py --batch-size=16 --device=cuda:0 --store=data/tokenizer-maltese --train=examples/multilanguage/languages/maltese.json
python3 cube/networks/tokenizer.py --batch-size=16 --device=cuda:0 --store=data/tokenizer-marathi --train=examples/multilanguage/languages/marathi.json
python3 cube/networks/tokenizer.py --batch-size=16 --device=cuda:0 --store=data/tokenizer-north_sami --train=examples/multilanguage/languages/north_sami.json
python3 cube/networks/tokenizer.py --batch-size=16 --device=cuda:0 --store=data/tokenizer-norwegian --train=examples/multilanguage/languages/norwegian.json
python3 cube/networks/tokenizer.py --batch-size=16 --device=cuda:0 --store=data/tokenizer-old_church_slavonic --train=examples/multilanguage/languages/old_church_slavonic.json
python3 cube/networks/tokenizer.py --batch-size=16 --device=cuda:0 --store=data/tokenizer-old_french --train=examples/multilanguage/languages/old_french.json
python3 cube/networks/tokenizer.py --batch-size=16 --device=cuda:0 --store=data/tokenizer-old_russian --train=examples/multilanguage/languages/old_russian.json
python3 cube/networks/tokenizer.py --batch-size=16 --device=cuda:0 --store=data/tokenizer-persian --train=examples/multilanguage/languages/persian.json
python3 cube/networks/tokenizer.py --batch-size=16 --device=cuda:0 --store=data/tokenizer-polish --train=examples/multilanguage/languages/polish.json
python3 cube/networks/tokenizer.py --batch-size=16 --device=cuda:0 --store=data/tokenizer-portuguese --train=examples/multilanguage/languages/portuguese.json
python3 cube/networks/tokenizer.py --batch-size=16 --device=cuda:0 --store=data/tokenizer-romanian --train=examples/multilanguage/languages/romanian.json
python3 cube/networks/tokenizer.py --batch-size=16 --device=cuda:0 --store=data/tokenizer-russian --train=examples/multilanguage/languages/russian.json
python3 cube/networks/tokenizer.py --batch-size=16 --device=cuda:0 --store=data/tokenizer-scottish_gaelic --train=examples/multilanguage/languages/scottish_gaelic.json
python3 cube/networks/tokenizer.py --batch-size=16 --device=cuda:0 --store=data/tokenizer-serbian --train=examples/multilanguage/languages/serbian.json
python3 cube/networks/tokenizer.py --batch-size=16 --device=cuda:0 --store=data/tokenizer-slovak --train=examples/multilanguage/languages/slovak.json
python3 cube/networks/tokenizer.py --batch-size=16 --device=cuda:0 --store=data/tokenizer-slovenian --train=examples/multilanguage/languages/slovenian.json
python3 cube/networks/tokenizer.py --batch-size=16 --device=cuda:0 --store=data/tokenizer-spanish --train=examples/multilanguage/languages/spanish.json
python3 cube/networks/tokenizer.py --batch-size=16 --device=cuda:0 --store=data/tokenizer-swedish --train=examples/multilanguage/languages/swedish.json
python3 cube/networks/tokenizer.py --batch-size=16 --device=cuda:0 --store=data/tokenizer-swedish_sign_language --train=examples/multilanguage/languages/swedish_sign_language.json
python3 cube/networks/tokenizer.py --batch-size=16 --device=cuda:0 --store=data/tokenizer-tamil --train=examples/multilanguage/languages/tamil.json
python3 cube/networks/tokenizer.py --batch-size=16 --device=cuda:0 --store=data/tokenizer-telugu --train=examples/multilanguage/languages/telugu.json
python3 cube/networks/tokenizer.py --batch-size=16 --device=cuda:0 --store=data/tokenizer-turkish --train=examples/multilanguage/languages/turkish.json
python3 cube/networks/tokenizer.py --batch-size=16 --device=cuda:0 --store=data/tokenizer-ukrainian --train=examples/multilanguage/languages/ukrainian.json
python3 cube/networks/tokenizer.py --batch-size=16 --device=cuda:0 --store=data/tokenizer-upper_sorbian --train=examples/multilanguage/languages/upper_sorbian.json
python3 cube/networks/tokenizer.py --batch-size=16 --device=cuda:0 --store=data/tokenizer-urdu --train=examples/multilanguage/languages/urdu.json
python3 cube/networks/tokenizer.py --batch-size=16 --device=cuda:0 --store=data/tokenizer-uyghur --train=examples/multilanguage/languages/uyghur.json
python3 cube/networks/tokenizer.py --batch-size=16 --device=cuda:0 --store=data/tokenizer-vietnamese --train=examples/multilanguage/languages/vietnamese.json
python3 cube/networks/tokenizer.py --batch-size=16 --device=cuda:0 --store=data/tokenizer-wolof --train=examples/multilanguage/languages/wolof.json

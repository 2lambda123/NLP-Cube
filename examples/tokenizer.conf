[TokenizerConfig]
char_embedding_size = 100
char_generic_feature_embedding_size = 5
char_generic_feature_vocabulary_size = 2
decoder_attribute_dropout = 0.33
decoder_hidden_size = 20
dropout_rate = 0
encoder_char_lstm_size = 200
encoder_word_lstm_size = 200
next_chars_embedding_size = 100
next_chars_window_size = 10
patience = 20
tokenize_maximum_sequence_length = 500

